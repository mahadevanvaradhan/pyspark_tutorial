{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Polynomial regression, revisited\n",
    "#We build on the material from Week 3, where we wrote the function to produce an DataFrame with columns containing the powers of a given input. \n",
    "#Copy and paste the function polynomial_dataframe from Week 3:\n",
    "def polynomial_dataframe(feature, degree): # feature is pandas.Series type\n",
    "    # assume that degree >= 1\n",
    "    # initialize the dataframe:\n",
    "    poly_dataframe = pd.DataFrame()\n",
    "    # and set poly_dataframe['power_1'] equal to the passed feature\n",
    "    poly_dataframe['power_1'] = feature\n",
    "    # first check if degree > 1\n",
    "    if degree > 1:\n",
    "        # then loop over the remaining degrees:\n",
    "        for power in range(2, degree+1):\n",
    "            # first we'll give the column a name:\n",
    "            name = 'power_' + str(power)\n",
    "            # assign poly_dataframe[name] to be feature^power; use apply(*)\n",
    "            poly_dataframe[name] = feature.apply(lambda x: x**power) #Result will be wrong if we use np.power(feature, power)\n",
    "    return poly_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1857862656"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.power(430, 10)#The result is not right. You can tell that np.power is making a mistake here. See below for solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.161148231328423e+26"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(np.log(430)*10) # This will reolve the issue and the result as below is right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "216114823132842490000000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "430**10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's use matplotlib to visualize what a polynomial regression looks like on the house data.\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>...</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19452</th>\n",
       "      <td>3980300371</td>\n",
       "      <td>20140926T000000</td>\n",
       "      <td>142000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>290</td>\n",
       "      <td>20875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>290</td>\n",
       "      <td>0</td>\n",
       "      <td>1963</td>\n",
       "      <td>0</td>\n",
       "      <td>98024</td>\n",
       "      <td>47.5308</td>\n",
       "      <td>-121.888</td>\n",
       "      <td>1620</td>\n",
       "      <td>22850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15381</th>\n",
       "      <td>2856101479</td>\n",
       "      <td>20140701T000000</td>\n",
       "      <td>276000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.75</td>\n",
       "      <td>370</td>\n",
       "      <td>1801</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>370</td>\n",
       "      <td>0</td>\n",
       "      <td>1923</td>\n",
       "      <td>0</td>\n",
       "      <td>98117</td>\n",
       "      <td>47.6778</td>\n",
       "      <td>-122.389</td>\n",
       "      <td>1340</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>1723049033</td>\n",
       "      <td>20140620T000000</td>\n",
       "      <td>245000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.75</td>\n",
       "      <td>380</td>\n",
       "      <td>15000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>380</td>\n",
       "      <td>0</td>\n",
       "      <td>1963</td>\n",
       "      <td>0</td>\n",
       "      <td>98168</td>\n",
       "      <td>47.4810</td>\n",
       "      <td>-122.323</td>\n",
       "      <td>1170</td>\n",
       "      <td>15000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18379</th>\n",
       "      <td>1222029077</td>\n",
       "      <td>20141029T000000</td>\n",
       "      <td>265000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>384</td>\n",
       "      <td>213444</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>384</td>\n",
       "      <td>0</td>\n",
       "      <td>2003</td>\n",
       "      <td>0</td>\n",
       "      <td>98070</td>\n",
       "      <td>47.4177</td>\n",
       "      <td>-122.491</td>\n",
       "      <td>1920</td>\n",
       "      <td>224341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4868</th>\n",
       "      <td>6896300380</td>\n",
       "      <td>20141002T000000</td>\n",
       "      <td>228000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>390</td>\n",
       "      <td>5900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>390</td>\n",
       "      <td>0</td>\n",
       "      <td>1953</td>\n",
       "      <td>0</td>\n",
       "      <td>98118</td>\n",
       "      <td>47.5260</td>\n",
       "      <td>-122.261</td>\n",
       "      <td>2170</td>\n",
       "      <td>6000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id             date     price  bedrooms  bathrooms  \\\n",
       "19452  3980300371  20140926T000000  142000.0         0       0.00   \n",
       "15381  2856101479  20140701T000000  276000.0         1       0.75   \n",
       "860    1723049033  20140620T000000  245000.0         1       0.75   \n",
       "18379  1222029077  20141029T000000  265000.0         0       0.75   \n",
       "4868   6896300380  20141002T000000  228000.0         0       1.00   \n",
       "\n",
       "       sqft_living  sqft_lot  floors  waterfront  view  ...  grade  \\\n",
       "19452          290     20875     1.0           0     0  ...      1   \n",
       "15381          370      1801     1.0           0     0  ...      5   \n",
       "860            380     15000     1.0           0     0  ...      5   \n",
       "18379          384    213444     1.0           0     0  ...      4   \n",
       "4868           390      5900     1.0           0     0  ...      4   \n",
       "\n",
       "       sqft_above  sqft_basement  yr_built  yr_renovated  zipcode      lat  \\\n",
       "19452         290              0      1963             0    98024  47.5308   \n",
       "15381         370              0      1923             0    98117  47.6778   \n",
       "860           380              0      1963             0    98168  47.4810   \n",
       "18379         384              0      2003             0    98070  47.4177   \n",
       "4868          390              0      1953             0    98118  47.5260   \n",
       "\n",
       "          long  sqft_living15  sqft_lot15  \n",
       "19452 -121.888           1620       22850  \n",
       "15381 -122.389           1340        5000  \n",
       "860   -122.323           1170       15000  \n",
       "18379 -122.491           1920      224341  \n",
       "4868  -122.261           2170        6000  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales = pd.read_csv('kc_house_data.csv')\n",
    "sales.sort_values(['sqft_living', 'price'])[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1180\n",
       "1    2570\n",
       "2     770\n",
       "3    1960\n",
       "4    1680\n",
       "Name: sqft_living, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales['sqft_living'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>power_1</th>\n",
       "      <th>power_2</th>\n",
       "      <th>power_3</th>\n",
       "      <th>power_4</th>\n",
       "      <th>power_5</th>\n",
       "      <th>power_6</th>\n",
       "      <th>power_7</th>\n",
       "      <th>power_8</th>\n",
       "      <th>power_9</th>\n",
       "      <th>power_10</th>\n",
       "      <th>power_11</th>\n",
       "      <th>power_12</th>\n",
       "      <th>power_13</th>\n",
       "      <th>power_14</th>\n",
       "      <th>power_15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19452</th>\n",
       "      <td>290</td>\n",
       "      <td>84100</td>\n",
       "      <td>24389000</td>\n",
       "      <td>7072810000</td>\n",
       "      <td>2051114900000</td>\n",
       "      <td>594823321000000</td>\n",
       "      <td>172498763090000000</td>\n",
       "      <td>50024641296100000000</td>\n",
       "      <td>14507145975869000000000</td>\n",
       "      <td>4207072333002010000000000</td>\n",
       "      <td>1220050976570582900000000000</td>\n",
       "      <td>353814783205469041000000000000</td>\n",
       "      <td>102606287129586021890000000000000</td>\n",
       "      <td>29755823267579946348100000000000000</td>\n",
       "      <td>8629188747598184440949000000000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15381</th>\n",
       "      <td>370</td>\n",
       "      <td>136900</td>\n",
       "      <td>50653000</td>\n",
       "      <td>18741610000</td>\n",
       "      <td>6934395700000</td>\n",
       "      <td>2565726409000000</td>\n",
       "      <td>949318771330000000</td>\n",
       "      <td>351247945392100000000</td>\n",
       "      <td>129961739795077000000000</td>\n",
       "      <td>48085843724178490000000000</td>\n",
       "      <td>17791762177946041300000000000</td>\n",
       "      <td>6582952005840035281000000000000</td>\n",
       "      <td>2435692242160813053970000000000000</td>\n",
       "      <td>901206129599500829968900000000000000</td>\n",
       "      <td>333446267951815307088493000000000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>380</td>\n",
       "      <td>144400</td>\n",
       "      <td>54872000</td>\n",
       "      <td>20851360000</td>\n",
       "      <td>7923516800000</td>\n",
       "      <td>3010936384000000</td>\n",
       "      <td>1144155825920000000</td>\n",
       "      <td>434779213849600000000</td>\n",
       "      <td>165216101262848000000000</td>\n",
       "      <td>62782118479882240000000000</td>\n",
       "      <td>23857205022355251200000000000</td>\n",
       "      <td>9065737908494995456000000000000</td>\n",
       "      <td>3444980405228098273280000000000000</td>\n",
       "      <td>1309092553986677343846400000000000000</td>\n",
       "      <td>497455170514937390661632000000000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18379</th>\n",
       "      <td>384</td>\n",
       "      <td>147456</td>\n",
       "      <td>56623104</td>\n",
       "      <td>21743271936</td>\n",
       "      <td>8349416423424</td>\n",
       "      <td>3206175906594816</td>\n",
       "      <td>1231171548132409344</td>\n",
       "      <td>472769874482845188096</td>\n",
       "      <td>181543631801412552228864</td>\n",
       "      <td>69712754611742420055883776</td>\n",
       "      <td>26769697770909089301459369984</td>\n",
       "      <td>10279563944029090291760398073856</td>\n",
       "      <td>3947352554507170672035992860360704</td>\n",
       "      <td>1515783380930753538061821258378510336</td>\n",
       "      <td>582060818277409358615739363217347969024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4868</th>\n",
       "      <td>390</td>\n",
       "      <td>152100</td>\n",
       "      <td>59319000</td>\n",
       "      <td>23134410000</td>\n",
       "      <td>9022419900000</td>\n",
       "      <td>3518743761000000</td>\n",
       "      <td>1372310066790000000</td>\n",
       "      <td>535200926048100000000</td>\n",
       "      <td>208728361158759000000000</td>\n",
       "      <td>81404060851916010000000000</td>\n",
       "      <td>31747583732247243900000000000</td>\n",
       "      <td>12381557655576425121000000000000</td>\n",
       "      <td>4828807485674805797190000000000000</td>\n",
       "      <td>1883234919413174260904100000000000000</td>\n",
       "      <td>734461618571137961752599000000000000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       power_1  power_2   power_3      power_4        power_5  \\\n",
       "19452      290    84100  24389000   7072810000  2051114900000   \n",
       "15381      370   136900  50653000  18741610000  6934395700000   \n",
       "860        380   144400  54872000  20851360000  7923516800000   \n",
       "18379      384   147456  56623104  21743271936  8349416423424   \n",
       "4868       390   152100  59319000  23134410000  9022419900000   \n",
       "\n",
       "                power_6              power_7                power_8  \\\n",
       "19452   594823321000000   172498763090000000   50024641296100000000   \n",
       "15381  2565726409000000   949318771330000000  351247945392100000000   \n",
       "860    3010936384000000  1144155825920000000  434779213849600000000   \n",
       "18379  3206175906594816  1231171548132409344  472769874482845188096   \n",
       "4868   3518743761000000  1372310066790000000  535200926048100000000   \n",
       "\n",
       "                        power_9                    power_10  \\\n",
       "19452   14507145975869000000000   4207072333002010000000000   \n",
       "15381  129961739795077000000000  48085843724178490000000000   \n",
       "860    165216101262848000000000  62782118479882240000000000   \n",
       "18379  181543631801412552228864  69712754611742420055883776   \n",
       "4868   208728361158759000000000  81404060851916010000000000   \n",
       "\n",
       "                            power_11                          power_12  \\\n",
       "19452   1220050976570582900000000000    353814783205469041000000000000   \n",
       "15381  17791762177946041300000000000   6582952005840035281000000000000   \n",
       "860    23857205022355251200000000000   9065737908494995456000000000000   \n",
       "18379  26769697770909089301459369984  10279563944029090291760398073856   \n",
       "4868   31747583732247243900000000000  12381557655576425121000000000000   \n",
       "\n",
       "                                 power_13  \\\n",
       "19452   102606287129586021890000000000000   \n",
       "15381  2435692242160813053970000000000000   \n",
       "860    3444980405228098273280000000000000   \n",
       "18379  3947352554507170672035992860360704   \n",
       "4868   4828807485674805797190000000000000   \n",
       "\n",
       "                                    power_14  \\\n",
       "19452    29755823267579946348100000000000000   \n",
       "15381   901206129599500829968900000000000000   \n",
       "860    1309092553986677343846400000000000000   \n",
       "18379  1515783380930753538061821258378510336   \n",
       "4868   1883234919413174260904100000000000000   \n",
       "\n",
       "                                      power_15  \n",
       "19452    8629188747598184440949000000000000000  \n",
       "15381  333446267951815307088493000000000000000  \n",
       "860    497455170514937390661632000000000000000  \n",
       "18379  582060818277409358615739363217347969024  \n",
       "4868   734461618571137961752599000000000000000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#What does the return of function polynomial_Datafram look like?\n",
    "poly15_dataframe = polynomial_dataframe(sales['sqft_living'], 15)\n",
    "poly15_dataframe.sort_values(['power_1']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>...</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19452</th>\n",
       "      <td>3980300371</td>\n",
       "      <td>20140926T000000</td>\n",
       "      <td>142000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>290</td>\n",
       "      <td>20875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>290</td>\n",
       "      <td>0</td>\n",
       "      <td>1963</td>\n",
       "      <td>0</td>\n",
       "      <td>98024</td>\n",
       "      <td>47.5308</td>\n",
       "      <td>-121.888</td>\n",
       "      <td>1620</td>\n",
       "      <td>22850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15381</th>\n",
       "      <td>2856101479</td>\n",
       "      <td>20140701T000000</td>\n",
       "      <td>276000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.75</td>\n",
       "      <td>370</td>\n",
       "      <td>1801</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>370</td>\n",
       "      <td>0</td>\n",
       "      <td>1923</td>\n",
       "      <td>0</td>\n",
       "      <td>98117</td>\n",
       "      <td>47.6778</td>\n",
       "      <td>-122.389</td>\n",
       "      <td>1340</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id             date     price  bedrooms  bathrooms  \\\n",
       "19452  3980300371  20140926T000000  142000.0         0       0.00   \n",
       "15381  2856101479  20140701T000000  276000.0         1       0.75   \n",
       "\n",
       "       sqft_living  sqft_lot  floors  waterfront  view  ...  grade  \\\n",
       "19452          290     20875     1.0           0     0  ...      1   \n",
       "15381          370      1801     1.0           0     0  ...      5   \n",
       "\n",
       "       sqft_above  sqft_basement  yr_built  yr_renovated  zipcode      lat  \\\n",
       "19452         290              0      1963             0    98024  47.5308   \n",
       "15381         370              0      1923             0    98117  47.6778   \n",
       "\n",
       "          long  sqft_living15  sqft_lot15  \n",
       "19452 -121.888           1620       22850  \n",
       "15381 -122.389           1340        5000  \n",
       "\n",
       "[2 rows x 21 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#As in Week 3, we will use the sqft_living variable. \n",
    "#For plotting purposes (connecting the dots), you'll need to sort by the values of sqft_living. For houses with identical square footage, we break the tie by their prices.\n",
    "sales = sales.sort_values(['sqft_living', 'price'])\n",
    "sales[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us revisit the 15th-order polynomial model using the 'sqft_living' input. Generate polynomial features up to degree 15 using polynomial_dataframe() and fit a model with these features. \n",
    "#When fitting the model, use an L2 penalty of 1.5e-5:\n",
    "l2_small_penalty = 1.5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With the L2 penalty specified above, write a function to fit the model and print out the learned weights.\n",
    "def polynomial_ridge_regression(data, deg, output, l2_penalty):\n",
    "    poly_dataframe = polynomial_dataframe(data['sqft_living'], deg)\n",
    "    my_features = poly_dataframe.columns.values.tolist()\n",
    "    poly_dataframe[output] = data[output]\n",
    "    reg = linear_model.Ridge (alpha = l2_penalty, normalize=True)\n",
    "    return reg.fit(poly_dataframe[my_features], poly_dataframe[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = polynomial_ridge_regression(sales, 15, 'price', 1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUIZ QUESTION: What's the learned value for the coefficient of feature power_1(using ridge regressin)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.74425503e+02, -7.83644074e-02,  3.73280427e-05, -2.85554311e-09,\n",
       "       -2.37487557e-13,  9.70529709e-18,  1.77791834e-21,  9.76993158e-26,\n",
       "        7.78897289e-31, -3.83833667e-34, -4.52940095e-38, -3.16694117e-42,\n",
       "       -1.24102475e-46,  4.53456467e-51,  1.63425896e-54])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#QUIZ QUESTION: What's the learned value for the coefficient of feature power_1(using ridge regressin)?\n",
    "model.coef_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-43580.74309447] [[280.6235679]]\n"
     ]
    }
   ],
   "source": [
    "#What's the learned value for the coefficient of feature power_1 using linear regression?\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "poly1_dataframe = polynomial_dataframe(sales['sqft_living'], 1)\n",
    "poly1_dataframe['price'] = sales['price']\n",
    "poly1_dataframe.head()\n",
    "model1_linear = LinearRegression()\n",
    "model1_linear.fit(poly1_dataframe['power_1'].values.reshape(-1,1), poly1_dataframe['price'].values.reshape(-1,1))\n",
    "print (model1_linear.intercept_, model1_linear.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Observe overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from Week 3 that the polynomial fit of degree 15 changed wildly whenever the data changed. \n",
    "In particular, when we split the sales data into four subsets and fit the model of degree 15, the result came out to be very different for each subset. The model had a high variance. We will see in a moment that ridge regression reduces such variance. But first, we must reproduce the experiment we did in Week 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File wk3_kc_house_set_1_data.csv does not exist: 'wk3_kc_house_set_1_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-fe8b942ae9c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#First load 4 sets of data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mset_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'wk3_kc_house_set_1_data.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mset_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'wk3_kc_house_set_2_data.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mset_3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'wk3_kc_house_set_3_data.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mset_4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'wk3_kc_house_set_4_data.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1891\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File wk3_kc_house_set_1_data.csv does not exist: 'wk3_kc_house_set_1_data.csv'"
     ]
    }
   ],
   "source": [
    "#First load 4 sets of data\n",
    "set_1 = pd.read_csv('wk3_kc_house_set_1_data.csv')\n",
    "set_2 = pd.read_csv('wk3_kc_house_set_2_data.csv')\n",
    "set_3 = pd.read_csv('wk3_kc_house_set_3_data.csv')\n",
    "set_4 = pd.read_csv('wk3_kc_house_set_4_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_poly_predictions(data, model):\n",
    "    poly_dataframe = polynomial_dataframe(data['sqft_living'], 15)\n",
    "    plt.plot(poly_dataframe['power_1'],data['price'],'.',\n",
    "        poly_dataframe['power_1'], model.predict(poly_dataframe),'-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Next, fit a 15th degree polynomial on set_1, set_2, set_3, and set_4, using 'sqft_living' to predict prices. \n",
    "Print the weights and make a plot of the resulting model.\n",
    "Hint: When calling linear_regression, use the same L2 penalty as before (i.e. l2_small_penalty). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_set1 = polynomial_ridge_regression(set_1, 15, 'price', 1e-9)\n",
    "print('intercept of model 1: ',model_set1.intercept_ )\n",
    "print('coefficients of model 1: ',model_set1.coef_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a model using set_1 with degree 15 and l2 norm penalty 1e-9\n",
    "# Call function polynomial_ridge_regression\n",
    "model1 = polynomial_ridge_regression(set_1, 15, 'price', 1e-9)\n",
    "print('intercept of model 1: ',model1.intercept_ )\n",
    "print('coefficients of model 1: ',model1.coef_ )\n",
    "plot_poly_predictions(set_1, model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a model using set_1 with degree 15 and l2 norm penalty 1e-9\n",
    "# not using function polynomial_ridge_regression\n",
    "set_1 = pd.read_csv('wk3_kc_house_set_1_data.csv')\n",
    "poly15_dataframe = polynomial_dataframe(set_1['sqft_living'], 15)\n",
    "my_features = poly15_dataframe.columns.values.tolist()\n",
    "poly15_dataframe['price'] = set_1['price']\n",
    "model15_set1 = linear_model.Ridge (alpha = 1e-9, normalize=True)\n",
    "model15_set1.fit(poly15_dataframe[my_features], poly15_dataframe['price'])\n",
    "print('intercept of model 15: ',model15_set1.intercept_ )\n",
    "print('coefficients of model 15: ',model15_set1.coef_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a model using set_2 with degree 15 and l2 norm penalty 1e-9\n",
    "# Call function polynomial_ridge_regression\n",
    "model2 = polynomial_ridge_regression(set_2, 15, 'price', 1e-9)\n",
    "print('intercept of model 2: ',model2.intercept_ )\n",
    "print('coefficients of model 2: ',model2.coef_ )\n",
    "plot_poly_predictions(set_2, model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a model using set_3 with degree 15 and l2 norm penalty 1e-9\n",
    "# Call function polynomial_ridge_regression\n",
    "model3 = polynomial_ridge_regression(set_3, 15, 'price', 1e-9)\n",
    "print('intercept of model 3: ',model3.intercept_ )\n",
    "print('coefficients of model 3: ',model3.coef_ )\n",
    "plot_poly_predictions(set_3, model3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a model using set_4 with degree 15 and l2 norm penalty 1e-9\n",
    "# Call function polynomial_ridge_regression\n",
    "model4 = polynomial_ridge_regression(set_4, 15, 'price', 1e-9)\n",
    "print('intercept of model 4: ',model4.intercept_ )\n",
    "print('coefficients of model 4: ',model4.coef_ )\n",
    "plot_poly_predictions(set_4, model4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The four curves should differ from one another a lot, as should the coefficients you learned.\n",
    "\n",
    "8 Quiz Question: For the models learned in each of these training sets, what are the smallest and largest values you learned for the coefficient of feature power_1? (For the purpose of answering this question, negative numbers are considered \"smaller\" than positive numbers. So -5 is smaller than -3, and -3 is smaller than 5 and so forth.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "8.59362655e+02-1.11944566e+03\n",
    "#the largest coefficient of power_1: 1.11944566e+03\n",
    "#the smallest coefficient of power_1: -7.55395955e+02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1.11944566e+03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge regression comes to rescue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9 Generally, whenever we see weights change so much in response to change in data, we believe the variance of our estimate to be large. Ridge regression aims to address this issue by penalizing \"large\" weights. (The weights looked quite small, but they are not that small because 'sqft_living' input is in the order of thousands.)\n",
    "\n",
    "l2_large_penalty=1.23e2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a model using set_1 with degree 15 and l2 norm penalty 1.23e2\n",
    "# Call function polynomial_ridge_regression\n",
    "model1_2 = polynomial_ridge_regression(set_1, 15, 'price', 1.23e2)\n",
    "print('intercept of model 1_2: ',model1_2.intercept_ )\n",
    "print('coefficients of model 1_2: ',model1_2.coef_ )\n",
    "plot_poly_predictions(set_1, model1_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a model using set_2 with degree 15 and l2 norm penalty 1.23e2\n",
    "# Call function polynomial_ridge_regression\n",
    "model2_2 = polynomial_ridge_regression(set_2, 15, 'price', 1.23e2)\n",
    "print('intercept of model 2_2: ',model2_2.intercept_ )\n",
    "print('coefficients of model 2_2: ',model2_2.coef_ )\n",
    "plot_poly_predictions(set_2, model2_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a model using set_3 with degree 15 and l2 norm penalty 1.23e2\n",
    "# Call function polynomial_ridge_regression\n",
    "model3_2 = polynomial_ridge_regression(set_3, 15, 'price', 1.23e2)\n",
    "print('intercept of model 3_2: ',model3_2.intercept_ )\n",
    "print('coefficients of model 3_2: ',model3_2.coef_ )\n",
    "plot_poly_predictions(set_3, model3_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a model using set_4 with degree 15 and l2 norm penalty 1.23e2\n",
    "# Call function polynomial_ridge_regression\n",
    "model4_2 = polynomial_ridge_regression(set_4, 15, 'price', 1.23e2)\n",
    "print('intercept of model 4_2: ',model4_2.intercept_ )\n",
    "print('coefficients of model 4_2: ',model4_2.coef_ )\n",
    "plot_poly_predictions(set_4, model4_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These curves should vary a lot less, now that you applied a high degree of regularization.\n",
    "\n",
    "***QUIZ QUESTION:  For the models learned with the high level of regularization in each of these training sets, what are the smallest and largest values you learned for the coefficient of feature `power_1`?*** (For the purpose of answering this question, negative numbers are considered \"smaller\" than positive numbers. So -5 is smaller than -3, and -3 is smaller than 5 and so forth.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the smallest values I learned for the coefficient of feature power_1\n",
    "2.08596194e+00\n",
    "\n",
    "#the largest values I learned for the coefficient of feature power_1\n",
    "2.32806803e+00\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting an L2 penalty via cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12.. Just like the polynomial degree, the L2 penalty is a \"magic\" parameter we need to select. We could use the validation set approach as we did in the last module, but that approach has a major disadvantage: it leaves fewer observations available for training. Cross-validation seeks to overcome this issue by using all of the training set in a smart way.\n",
    "\n",
    "We will implement a kind of cross-validation called k-fold cross-validation. The method gets its name because it involves dividing the training set into k segments of roughtly equal size. Similar to the validation set method, we measure the validation error with one of the segments designated as the validation set. The major difference is that we repeat the process k times as follows:\n",
    "\n",
    "Set aside segment 0 as the validation set, and fit a model on rest of data, and evalutate it on this validation set\n",
    "Set aside segment 1 as the validation set, and fit a model on rest of data, and evalutate it on this validation set\n",
    "...\n",
    "Set aside segment k-1 as the validation set, and fit a model on rest of data, and evalutate it on this validation set\n",
    "After this process, we compute the average of the k validation errors, and use it as an estimate of the generalization error. Notice that all observations are used for both training and validation, as we iterate over segments of data.\n",
    "\n",
    "13.. To estimate the generalization error well, it is crucial to shuffle the training data before dividing them into segments. We reserve 10% of the data as the test set and randomly shuffle the remainder. Le'ts call the shuffled data 'train_valid_shuffled'.\n",
    "\n",
    "For the purpose of this assignment, let us download the csv file containing pre-shuffled rows of training and validation sets combined:\n",
    "\n",
    "In practice, you would shuffle the rows with a dynamically determined random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_valid_shuffled = pd.read_csv('wk3_kc_house_train_valid_shuffled.csv')\n",
    "test = pd.read_csv('wk3_kc_house_test_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14.. Divide the combined training and validation set into equal segments. Each segment should receive n/k elements, where n is the number of observations in the training set and k is the number of segments. Since the segment 0 starts at index 0 and contains n/k elements, it ends at index (n/k)-1. The segment 1 starts where the segment 0 left off, at index (n/k). With n/k elements, the segment 1 ends at index (n*2/k)-1. Continuing in this fashion, we deduce that the segment i starts at index (n*i/k) and ends at (n*(i+1)/k)-1.\n",
    "\n",
    "With this pattern in mind, we write a short loop that prints the starting and ending indices of each segment, just to make sure you are getting the splits right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n = len(train_valid_shuffled)\n",
    "k = 10 # 10-fold cross-validation\n",
    "\n",
    "for i in range(k):\n",
    "    start = (n*i)/k\n",
    "    end = (n*(i+1))/k-1\n",
    "    print (i, (start, end))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us familiarize ourselves with array slicing with Pandas. To extract a continuous slice from a DataFrame, use colon in square brackets. For instance, the following cell extracts rows 0 to 9 of train_valid_shuffled. Notice that the first index (0) is included in the slice but the last index (10) is omitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_valid_shuffled[0:10] # select rows 0 to 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the observations are grouped into 10 segments, the segment i is given by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = int((n*i)/10)\n",
    "end = int((n*(i+1))/10)\n",
    "train_valid_shuffled[start:end+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_valid_shuffled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us extract individual segments with array slicing. Consider the scenario where we group the houses in the train_valid_shuffled dataframe into k=10 segments of roughly equal size, with starting and ending indices computed as above. Extract the fourth segment (segment 3) and assign it to a variable called validation4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(train_valid_shuffled)\n",
    "k = 10 # 10-fold cross-validation\n",
    "i=3\n",
    "start = int((n*i)/10)\n",
    "end = int((n*(i+1))/10)\n",
    "validation4 = train_valid_shuffled[start:end+1]\n",
    "validation4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify that we have the right elements extracted, run the following cell, which computes the average price of the fourth segment. When rounded to nearest whole number, the average should be $536,234."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (int(round(validation4['price'].mean(), 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After designating one of the k segments as the validation set, we train a model using the rest of the data.\n",
    "\n",
    "Meanwhile, to choose the remainder of the data that's not part of the segment i, we select two slices (0:start) and (end+1:n) and paste them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_valid_shuffled[0:start].append(train_valid_shuffled[end+1:n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15.. Now we are ready to implement k-fold cross-validation. Write a function that computes k validation errors by designating each of the k segments as the validation set. It accepts as parameters (i) k, (ii) l2_penalty, (iii) dataframe containing input features (e.g. poly15_data) and (iv) column of output values (e.g. price). The function returns the average validation error using k segments as validation sets. We shall assume that the input dataframe does not contain the output column.\n",
    "\n",
    "For each i in [0, 1, ... k-1]:\n",
    "\n",
    "Compute starting and ending indices of segment i and call 'start' and 'end'\n",
    "Form validation set by taking a slice (start:end+1) from the data.\n",
    "Form training set by appending slice (end+1:n) to the end of slice (0:start).\n",
    "Train a linear model using training set just formed, with a given l2_penalty\n",
    "Compute validation error (RSS) using validation set just formed\n",
    "e.g. in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cross_validation(k, l2_penalty, data, output):\n",
    "    RSS_all = []\n",
    "    for i in range(k):\n",
    "        #identify train data and valid data\n",
    "        n = len(data)\n",
    "        start = int((n*i)/k)\n",
    "        end = int((n*(i+1))/k)\n",
    "        train_data = data[0:start].append(data[end+1:n])\n",
    "        valid_data = data[start:end+1]\n",
    "        poly_dataframe_train = polynomial_dataframe(train_data['sqft_living'], 15)\n",
    "        my_features_train = poly_dataframe_train.columns.values.tolist()\n",
    "        poly_dataframe_train[output] = train_data[output]\n",
    "        #fit model\n",
    "        reg = linear_model.Ridge (alpha = l2_penalty, normalize=True)\n",
    "        model = reg.fit(poly_dataframe_train[my_features_train], poly_dataframe_train[output])\n",
    "        #get the predictions of valid data\n",
    "        poly_dataframe_valid = polynomial_dataframe(valid_data['sqft_living'], 15)\n",
    "        predictions_valid = model.predict(poly_dataframe_valid)\n",
    "        outcomes_valid = valid_data[output]\n",
    "        #calculate residuals of predictions based on valid data\n",
    "        residuals_valid = predictions_valid - outcomes_valid\n",
    "        #calculate RSS of the predictions\n",
    "        RSS_valid = (residuals_valid * residuals_valid).sum()\n",
    "        RSS_all.append(RSS_valid)\n",
    "    #print(\"RSS_all\",RSS_all)\n",
    "    RSS_avg = sum(RSS_all)/len(RSS_all)\n",
    "    #print(\"RSS_avg\",RSS_avg)\n",
    "    return RSS_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold_cross_validation(10, 10, train_valid_shuffled, 'price')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16.. Once we have a function to compute the average validation error for a model, we can write a loop to find the model that minimizes the average validation error. Write a loop that does the following:\n",
    "\n",
    "We will again be aiming to fit a 15th-order polynomial model using the sqft_living input\n",
    "For each l2_penalty in [10^3, 10^3.5, 10^4, 10^4.5, ..., 10^9] (to get this in Python, you can use this Numpy function: np.logspace(3, 9, num=13).): Run 10-fold cross-validation with l2_penalty.\n",
    "Report which L2 penalty produced the lowest average validation error.\n",
    "Note: since the degree of the polynomial is now fixed to 15, to make things faster, you should generate polynomial features in advance and re-use them throughout the loop. Make sure to use train_valid_shuffled when generating polynomial features!\n",
    "\n",
    "17.. Quiz Question: What is the best value for the L2 penalty according to 10-fold validation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l2_penalty in np.logspace(3, 9, num=13):\n",
    "    print(l2_penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.power(10,3.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_valid_RSS = []\n",
    "all_l2 = []\n",
    "for l2_penalty in np.logspace(3, 9, num=13):\n",
    "    valid_RSS = k_fold_cross_validation(10, l2_penalty, train_valid_shuffled, 'price')\n",
    "    all_valid_RSS.append(valid_RSS)\n",
    "    all_l2.append(l2_penalty)\n",
    "pd_all = pd.DataFrame(\n",
    "{'RSS': all_valid_RSS,\n",
    "'l2': all_l2,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the miminum RSS and its l2 norm penalty\n",
    "#Question: What is the best value for the L2 penalty according to 10-fold validation?\n",
    "#Answer: 10^3\n",
    "#Solution 1:\n",
    "pd_all.sort_values(['RSS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution 2\n",
    "\n",
    "all_valid_RSS = []\n",
    "for l2_penalty in np.logspace(3, 9, num=13):\n",
    "    valid_RSS = k_fold_cross_validation(10, l2_penalty, train_valid_shuffled, 'price')\n",
    "    print(valid_RSS,l2_penalty)\n",
    "    all_valid_RSS.append(valid_RSS)\n",
    "all_valid_RSS.index(min(all_valid_RSS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "19 Quiz Question: Using the best L2 penalty found above, train a model using all training data. What is the RSS on the TEST data of the model you learn with this L2 penalty?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit a model\n",
    "model_alltrain = polynomial_ridge_regression(train_valid_shuffled, 15, 'price', 1000)\n",
    "#identfy x of test data\n",
    "poly_dataframe_test = polynomial_dataframe(test['sqft_living'], 15)\n",
    "#get the predictions of test data\n",
    "predictions_test = model_alltrain.predict(poly_dataframe_test)\n",
    "outcomes_test = test['price']\n",
    "#calculate the residuals of predictions based on test data\n",
    "residuals_test = predictions_test - outcomes_test\n",
    "#calculate RSS of the predictions\n",
    "RSS_test = (residuals_test * residuals_test).sum()\n",
    "print(RSS_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
